{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from time import sleep, time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"articles_info.csv\" # имя файла, в который будем сохранять результат\n",
    "driver_path = r\"C:\\Users\\User\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "base_dir= r\"C:\\Users\\User\\Desktop\" # укажите директорию, в которую будем сохранять файл\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\" #Ваш user-agent. Узнать можно тут https://юзерагент.рф, смотреть с браузера Chrome\n",
    "start_time = time() # время начала выполнения программы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_load_time(article_url, user_agent):\n",
    "    #будем ждать 3 секунды, иначе выводить exception и присваивать константное значение\n",
    "    try:\n",
    "        # меняем значение заголовка. По умолчанию указано, что это python-код\n",
    "        headers = {\n",
    "            \"User-Agent\": user_agent\n",
    "        }\n",
    "        # делаем запрос по url статьи article_url\n",
    "        response = requests.get(\n",
    "            article_url, headers=headers, stream=True, timeout=3.000\n",
    "        )\n",
    "        # получаем время загрузки страницы\n",
    "        load_time = response.elapsed.total_seconds()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        load_time = \">3\"\n",
    "    return load_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(output_list, filename, base_dir):\n",
    "    for row in output_list:\n",
    "        with open(Path(base_dir).joinpath(filename), \"a\") as csvfile:\n",
    "            fieldnames = [\"id\", \"load_time\", \"rank\", \"points\", \"title\", \"url\", \"num_comments\"]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_base(browser, page_number):\n",
    "    base_url = \"https://news.ycombinator.com/news?p={}\".format(page_number)\n",
    "    for connection_attempts in range(1,4): # совершаем 3 попытки подключения\n",
    "        try:\n",
    "            browser.get(base_url)\n",
    "            # ожидаем пока элемент table с id = 'hnmain' будет загружен на страницу\n",
    "            # затем функция вернет True иначе False \n",
    "            WebDriverWait(browser, 5).until(\n",
    "                EC.presence_of_element_located((By.ID, \"hnmain\"))\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error connecting to {}.\".format(base_url))\n",
    "            print(\"Attempt #{}.\".format(connection_attempts))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(html, user_agent):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    output_list = []\n",
    "    \n",
    "    # ищем в объекте soup object id, rank, score и title статьи\n",
    "    tr_blocks = soup.find_all(\"tr\", class_=\"athing\")\n",
    "    article = 0\n",
    "    \n",
    "    # начальный номер строки, где хранится дополнительная инфа (subtext)\n",
    "    subtext_row = 2 \n",
    "    \n",
    "    # понадобится для использования xpath, чтобы найти количество комментариев\n",
    "    dom = etree.HTML(str(soup))\n",
    "    \n",
    "    for tr in tr_blocks:\n",
    "        article_id = tr.get(\"id\") # id\n",
    "        article_url = tr.find_all(\"a\")[1][\"href\"]\n",
    "        \n",
    "        # иногда статья располагается не на внешнем сайте, а на ycombinator, тогда article_url у нее не полный, \n",
    "        # а добавочный, с параметрами. Например item?id=200933. Для этих случаев будем добавлять урл до полного\n",
    "        if \"item?id=\" in article_url or \"from?site=\" in article_url:\n",
    "            article_url = f\"https://news.ycombinator.com/{article_url}\"\n",
    "            \n",
    "        load_time = get_load_time(article_url, user_agent)\n",
    "        \n",
    "        # иногда рейтинга может не быть, поэтому воспользуемся try\n",
    "        try:\n",
    "            score = soup.find(id=f\"score_{article_id}\").string\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            score = \"0 points\"\n",
    "        \n",
    "        # иногда комментариев может не быть, поэтому воспользуемся try\n",
    "        try:\n",
    "            #print(f'//*[@id=\"hnmain\"]/tbody/tr[3]/td/table/tbody/tr[{subtext_row}]/td[2]/a[3]')\n",
    "            num_comments = dom.xpath(f'//*[@id=\"hnmain\"]/tbody/tr[3]/td/table/tbody/tr[{subtext_row}]/td[2]/a[3]')[0].text\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            num_comments = \"0 comments\"    \n",
    "        # следующий номер строки, где хранится дополнительная инфа (subtext)\n",
    "        subtext_row += 3\n",
    "\n",
    "        article_info = {\n",
    "            \"id\": article_id,\n",
    "            \"load_time\": load_time,\n",
    "            \"rank\": tr.span.string,\n",
    "            \"points\": score,\n",
    "            \"title\": tr.find(class_=\"storylink\").string,\n",
    "            \"url\": article_url,\n",
    "            \"num_comments\": num_comments\n",
    "        }\n",
    "\n",
    "        # добавляем информацию о статье в список\n",
    "        output_list.append(article_info)\n",
    "        article += 1\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting page 0...\n",
      "'NoneType' object has no attribute 'string'\n",
      "list index out of range\n",
      "getting page 1...\n",
      "'NoneType' object has no attribute 'string'\n",
      "list index out of range\n",
      "getting page 2...\n",
      "getting page 3...\n",
      "HTTPSConnectionPool(host='www.quantamagazine.org', port=443): Read timed out. (read timeout=3.0)\n",
      "HTTPSConnectionPool(host='telephoneworld.org', port=443): Read timed out. (read timeout=3.0)\n",
      "getting page 4...\n",
      "getting page 5...\n",
      "getting page 6...\n",
      "getting page 7...\n",
      "getting page 8...\n",
      "getting page 9...\n",
      "HTTPSConnectionPool(host='status.ovh.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000203ED751B50>, 'Connection to status.ovh.com timed out. (connect timeout=3.0)'))\n",
      "run time: 240.6899333000183 seconds\n"
     ]
    }
   ],
   "source": [
    "# инициализируем веб драйвер\n",
    "browser = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "# перебираем страницы и собираем нужную информацию\n",
    "for page_number in range(10):\n",
    "    print(\"getting page \" + str(page_number) + \"...\")\n",
    "    if connect_to_base(browser, page_number):\n",
    "        sleep(5)\n",
    "        output_list = parse_html(browser.page_source, user_agent)\n",
    "        write_to_file(output_list, filename, base_dir)\n",
    "\n",
    "    else:\n",
    "        print(\"Error connecting to hacker news\")\n",
    "    \n",
    "# завершаем работу драйвера\n",
    "browser.close()\n",
    "sleep(1)\n",
    "browser.quit()\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"run time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "\n",
    "def run_process(page_number, filename):\n",
    "    browser = webdriver.Chrome(executable_path=driver_path)\n",
    "    if connect_to_base(browser, page_number):\n",
    "        sleep(5)\n",
    "        output_list = parse_html(browser.page_source, user_agent)\n",
    "        write_to_file(output_list, filename, base_dir)\n",
    "        \n",
    "        browser.quit()\n",
    "    else:\n",
    "        print(\"Error connecting to hacker news\")\n",
    "        browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "\n",
      "Error connecting to https://news.ycombinator.com/news?p=4.\n",
      "Attempt #1.\n",
      "Message: \n",
      "\n",
      "Error connecting to https://news.ycombinator.com/news?p=6.\n",
      "Attempt #1.\n",
      "Message: \n",
      "\n",
      "Error connecting to https://news.ycombinator.com/news?p=7.\n",
      "Attempt #1.\n",
      "Message: \n",
      "\n",
      "Error connecting to https://news.ycombinator.com/news?p=9.\n",
      "Attempt #1.\n",
      "Message: \n",
      "\n",
      "Error connecting to https://news.ycombinator.com/news?p=8.\n",
      "Attempt #1.\n",
      "'NoneType' object has no attribute 'string'\n",
      "list index out of range\n",
      "'NoneType' object has no attribute 'string'\n",
      "list index out of range\n",
      "Message: \n",
      "\n",
      "Error connecting to https://news.ycombinator.com/news?p=8.\n",
      "Attempt #2.\n",
      "HTTPSConnectionPool(host='passionhacks.com', port=443): Max retries exceeded with url: /gradient-isometric-illustrations/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)')))\n",
      "HTTPSConnectionPool(host='ruqqus.com', port=443): Read timed out. (read timeout=3.0)\n",
      "HTTPSConnectionPool(host='ruqqus.com', port=443): Read timed out. (read timeout=3.0)\n",
      "HTTPSConnectionPool(host='support.weather.com', port=443): Max retries exceeded with url: /s/article/Webcam-hosting-service-is-discontinued-effective-October-21-2021?language=en_US (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)')))\n",
      "Elapsed run time: 99.83063769340515 seconds\n"
     ]
    }
   ],
   "source": [
    "futures = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for number in range(10):\n",
    "        futures.append(\n",
    "            executor.submit(run_process, number, filename)\n",
    "        )\n",
    "wait(futures)\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed run time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
